{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARec++: Comprehensive Metadata-Driven Cold-Start Recommendation\n",
    "\n",
    "**Research Question**: Can richer and more comprehensive metadata representations\n",
    "outperform the original MARec results in cold-start recommendation, without\n",
    "changing the backbone architecture?\n",
    "\n",
    "## Experiment Plan\n",
    "1. **Baseline reproduction** — MARec with top3 features (actors, directors, genres)\n",
    "2. **Tag modes** — no_tags, tags_train_users (leakage-safe), tags_full (upper bound)\n",
    "3. **Feature ablation** — individual and combined feature contributions\n",
    "4. **Control: random tag shuffle** — verify tags carry content signal, not density\n",
    "5. **Cold-item tag coverage** — analyze how many cold items have tags\n",
    "6. **Publication-quality tables & plots**\n",
    "\n",
    "All runs use **10 splits** with bootstrap CIs and paired significance tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                       \"numpy\", \"scipy\", \"pandas\", \"scikit-learn\", \"matplotlib\",\n",
    "                       \"seaborn\", \"bottleneck\", \"tabulate\"])"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os, time, json, warnings, copy, zipfile, urllib.request\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Set, Tuple\n",
    "from collections import Counter\n",
    "from itertools import product as iterproduct\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({\n",
    "    \"font.size\": 12, \"axes.titlesize\": 14, \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 10, \"ytick.labelsize\": 10,\n",
    "    \"figure.figsize\": (10, 6), \"figure.dpi\": 120,\n",
    "})\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_style(\"whitegrid\")\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import bottleneck as bn\n",
    "    _USE_BN = True\n",
    "except ImportError:\n",
    "    _USE_BN = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading (with user-tags fix)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "HETREC_URL = \"http://files.grouplens.org/datasets/hetrec2011/hetrec2011-movielens-2k-v2.zip\"\n",
    "\n",
    "def download_hetrec(data_dir):\n",
    "    data_dir = Path(data_dir)\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    raw_dir = data_dir / \"hetrec2011-movielens-2k-v2\"\n",
    "    if raw_dir.exists():\n",
    "        return raw_dir\n",
    "    ratings_file = data_dir / \"user_ratedmovies-timestamps.dat\"\n",
    "    if ratings_file.exists():\n",
    "        return data_dir\n",
    "    zip_path = data_dir / \"hetrec2011-movielens-2k-v2.zip\"\n",
    "    print(f\"Downloading HetRec 2011 to {zip_path}...\")\n",
    "    urllib.request.urlretrieve(HETREC_URL, str(zip_path))\n",
    "    with zipfile.ZipFile(str(zip_path), \"r\") as zf:\n",
    "        zf.extractall(str(data_dir))\n",
    "    if raw_dir.exists():\n",
    "        return raw_dir\n",
    "    elif ratings_file.exists():\n",
    "        return data_dir\n",
    "    raise RuntimeError(\"Extraction failed\")\n",
    "\n",
    "def read_dat(raw_dir, filename):\n",
    "    path = Path(raw_dir) / filename\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame()\n",
    "    with open(str(path), \"r\", encoding=\"latin-1\") as f:\n",
    "        text = f.read()\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    rows = [line.replace(\"\\r\", \"\").split(\"\\t\") for line in lines]\n",
    "    return pd.DataFrame(rows[1:], columns=rows[0])\n",
    "\n",
    "def load_all_dataframes(raw_dir):\n",
    "    dfs = {}\n",
    "    dfs[\"movies\"] = read_dat(raw_dir, \"movies.dat\")\n",
    "    dfs[\"ratings\"] = read_dat(raw_dir, \"user_ratedmovies-timestamps.dat\")\n",
    "    dfs[\"genres\"] = read_dat(raw_dir, \"movie_genres.dat\")\n",
    "    dfs[\"actors\"] = read_dat(raw_dir, \"movie_actors.dat\")\n",
    "    dfs[\"directors\"] = read_dat(raw_dir, \"movie_directors.dat\")\n",
    "    dfs[\"countries\"] = read_dat(raw_dir, \"movie_countries.dat\")\n",
    "    dfs[\"tags\"] = read_dat(raw_dir, \"movie_tags.dat\")\n",
    "    # FIX: Load per-user tag assignments (has userID column for leakage-safe modes)\n",
    "    dfs[\"user_tags\"] = read_dat(raw_dir, \"user_taggedmovies-timestamps.dat\")\n",
    "    if dfs[\"user_tags\"].empty:\n",
    "        dfs[\"user_tags\"] = read_dat(raw_dir, \"user_taggedmovies.dat\")\n",
    "    dfs[\"locations\"] = read_dat(raw_dir, \"movie_locations.dat\")\n",
    "    return dfs\n",
    "\n",
    "def binarize_and_kcore(df_ratings, threshold=3.0, min_user=5, min_item=5):\n",
    "    df = df_ratings.copy()\n",
    "    df[\"userID\"] = df[\"userID\"].astype(str)\n",
    "    df[\"movieID\"] = df[\"movieID\"].astype(str)\n",
    "    df[\"rating\"] = df[\"rating\"].astype(float)\n",
    "    df = df[df[\"rating\"] >= threshold].copy()\n",
    "    df = df.groupby([\"userID\", \"movieID\"])[\"rating\"].max().reset_index()\n",
    "    for _ in range(200):\n",
    "        n = len(df)\n",
    "        ic = df[\"movieID\"].value_counts()\n",
    "        df = df[df[\"movieID\"].isin(ic[ic >= min_item].index)]\n",
    "        uc = df[\"userID\"].value_counts()\n",
    "        df = df[df[\"userID\"].isin(uc[uc >= min_user].index)]\n",
    "        if len(df) == n:\n",
    "            break\n",
    "    users = sorted(df[\"userID\"].unique())\n",
    "    items = sorted(df[\"movieID\"].unique())\n",
    "    user2idx = {u: i for i, u in enumerate(users)}\n",
    "    item2idx = {m: i for i, m in enumerate(items)}\n",
    "    idx2item = {i: m for m, i in item2idx.items()}\n",
    "    rows = np.array([user2idx[u] for u in df[\"userID\"]])\n",
    "    cols = np.array([item2idx[m] for m in df[\"movieID\"]])\n",
    "    URM = sp.csr_matrix(\n",
    "        (np.ones(len(rows), dtype=np.float32), (rows, cols)),\n",
    "        shape=(len(users), len(items)))\n",
    "    return URM, user2idx, item2idx, idx2item, df\n",
    "\n",
    "def load_dataset(data_dir=\"hetrec_data\", threshold=3.0):\n",
    "    raw_dir = download_hetrec(data_dir)\n",
    "    dfs = load_all_dataframes(raw_dir)\n",
    "    URM, user2idx, item2idx, idx2item, df_filtered = binarize_and_kcore(\n",
    "        dfs[\"ratings\"], threshold=threshold)\n",
    "    n_u, n_i = URM.shape\n",
    "    density = URM.nnz / (n_u * n_i) * 100\n",
    "    print(f\"Dataset: {n_u} users x {n_i} items, {URM.nnz} interactions, density={density:.2f}%\")\n",
    "    return {\"URM\": URM, \"user2idx\": user2idx, \"item2idx\": item2idx,\n",
    "            \"idx2item\": idx2item, \"df_filtered\": df_filtered, \"dfs\": dfs,\n",
    "            \"n_users\": n_u, \"n_items\": n_i}\n",
    "\n",
    "print(\"Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Encoding (with tag-mode fix)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _build_feature_dict(df, id_col, val_col):\n",
    "    d = {}\n",
    "    for _, row in df.iterrows():\n",
    "        d.setdefault(str(row[id_col]), []).append(str(row[val_col]))\n",
    "    return d\n",
    "\n",
    "def encode_multihot(item_lists, min_count=2):\n",
    "    flat = [v for lst in item_lists for v in lst]\n",
    "    counter = Counter(flat)\n",
    "    valid = sorted(k for k, v in counter.items() if v >= min_count and k.strip())\n",
    "    if not valid:\n",
    "        return np.zeros((len(item_lists), 1), dtype=np.float32)\n",
    "    mlb = MultiLabelBinarizer(classes=valid, sparse_output=False)\n",
    "    return mlb.fit_transform(item_lists).astype(np.float32)\n",
    "\n",
    "def encode_tfidf(texts, max_features=10000):\n",
    "    if not any(t.strip() for t in texts):\n",
    "        return np.zeros((len(texts), 1), dtype=np.float32)\n",
    "    try:\n",
    "        pipe = Pipeline([(\"count\", CountVectorizer(max_features=max_features)),\n",
    "                         (\"tfidf\", TfidfTransformer())])\n",
    "        return pipe.fit_transform(texts).toarray().astype(np.float32)\n",
    "    except ValueError:\n",
    "        return np.zeros((len(texts), 1), dtype=np.float32)\n",
    "\n",
    "def encode_years(year_values):\n",
    "    arr = np.array(year_values, dtype=np.float32).reshape(-1, 1)\n",
    "    return MinMaxScaler().fit_transform(arr)\n",
    "\n",
    "def _filter_tags_train_pairs(df_tags, train_csr, user2idx, item2idx):\n",
    "    result = {}\n",
    "    for _, row in df_tags.iterrows():\n",
    "        uid = str(row.get(\"userID\", \"\"))\n",
    "        mid = str(row.get(\"movieID\", \"\"))\n",
    "        tid = str(row.get(\"tagID\", \"\"))\n",
    "        if uid not in user2idx or mid not in item2idx:\n",
    "            continue\n",
    "        u_idx, i_idx = user2idx[uid], item2idx[mid]\n",
    "        if train_csr[u_idx, i_idx] > 0:\n",
    "            result.setdefault(mid, []).append(tid)\n",
    "    return result\n",
    "\n",
    "def _filter_tags_train_users(df_tags, train_csr, user2idx, item2idx):\n",
    "    train_users = set(train_csr.tocoo().row)\n",
    "    result = {}\n",
    "    for _, row in df_tags.iterrows():\n",
    "        uid = str(row.get(\"userID\", \"\"))\n",
    "        mid = str(row.get(\"movieID\", \"\"))\n",
    "        tid = str(row.get(\"tagID\", \"\"))\n",
    "        if uid not in user2idx or mid not in item2idx:\n",
    "            continue\n",
    "        u_idx = user2idx[uid]\n",
    "        if u_idx in train_users:\n",
    "            result.setdefault(mid, []).append(tid)\n",
    "    return result\n",
    "\n",
    "def build_feature_matrices(dfs, item2idx, idx2item, n_items,\n",
    "                           tag_mode=\"no_tags\", train_csr=None,\n",
    "                           user2idx=None, min_count=2,\n",
    "                           shuffle_tags=False):\n",
    "    assert tag_mode in (\"no_tags\", \"tags_train_only\", \"tags_train_users\",\n",
    "                        \"tags_train_pairs\", \"tags_full\")\n",
    "    if tag_mode in (\"tags_train_only\", \"tags_train_users\", \"tags_train_pairs\"):\n",
    "        assert train_csr is not None and user2idx is not None\n",
    "\n",
    "    dict_genres = _build_feature_dict(dfs[\"genres\"], \"movieID\", \"genre\")\n",
    "    dict_actors = _build_feature_dict(dfs[\"actors\"], \"movieID\", \"actorName\")\n",
    "    dict_directors = _build_feature_dict(dfs[\"directors\"], \"movieID\", \"directorName\")\n",
    "    dict_countries = _build_feature_dict(dfs[\"countries\"], \"movieID\", \"country\")\n",
    "\n",
    "    # FIX: Use dfs[\"user_tags\"] for leakage-safe modes\n",
    "    if tag_mode == \"tags_full\":\n",
    "        dict_tags = _build_feature_dict(dfs[\"tags\"], \"movieID\", \"tagID\")\n",
    "    elif tag_mode == \"tags_train_pairs\":\n",
    "        dict_tags = _filter_tags_train_pairs(\n",
    "            dfs.get(\"user_tags\", dfs[\"tags\"]), train_csr, user2idx, item2idx)\n",
    "    elif tag_mode == \"tags_train_users\":\n",
    "        dict_tags = _filter_tags_train_users(\n",
    "            dfs.get(\"user_tags\", dfs[\"tags\"]), train_csr, user2idx, item2idx)\n",
    "    elif tag_mode == \"tags_train_only\":\n",
    "        dict_tags = _filter_tags_train_pairs(\n",
    "            dfs.get(\"user_tags\", dfs[\"tags\"]), train_csr, user2idx, item2idx)\n",
    "    else:\n",
    "        dict_tags = {}\n",
    "\n",
    "    df_movies = dfs[\"movies\"]\n",
    "    dict_years = {}\n",
    "    for _, row in df_movies.iterrows():\n",
    "        mid = str(row[\"id\"])\n",
    "        try:\n",
    "            dict_years[mid] = int(row[\"year\"])\n",
    "        except (ValueError, KeyError):\n",
    "            dict_years[mid] = 2000\n",
    "\n",
    "    dict_locs = [{}, {}, {}]\n",
    "    if len(dfs[\"locations\"]) > 0:\n",
    "        lc = dfs[\"locations\"].columns.tolist()\n",
    "        for _, row in dfs[\"locations\"].iterrows():\n",
    "            mid = str(row[lc[0]])\n",
    "            for lvl in range(min(3, len(lc) - 1)):\n",
    "                dict_locs[lvl].setdefault(mid, []).append(str(row[lc[lvl + 1]]))\n",
    "\n",
    "    feat_lists = {\"genres\": [], \"actors\": [], \"directors\": [], \"countries\": [],\n",
    "                  \"loc1\": [], \"loc2\": [], \"loc3\": [], \"years\": []}\n",
    "    loc_texts = []\n",
    "    for i in range(n_items):\n",
    "        mid = str(idx2item[i])\n",
    "        feat_lists[\"genres\"].append(dict_genres.get(mid, []))\n",
    "        feat_lists[\"actors\"].append(dict_actors.get(mid, []))\n",
    "        feat_lists[\"directors\"].append(dict_directors.get(mid, []))\n",
    "        feat_lists[\"countries\"].append(dict_countries.get(mid, []))\n",
    "        feat_lists[\"loc1\"].append(dict_locs[0].get(mid, []))\n",
    "        feat_lists[\"loc2\"].append(dict_locs[1].get(mid, []))\n",
    "        feat_lists[\"loc3\"].append(dict_locs[2].get(mid, []))\n",
    "        feat_lists[\"years\"].append(dict_years.get(mid, 2000))\n",
    "        loc_parts = [\" \".join(dict_locs[j].get(mid, [])) for j in range(3)]\n",
    "        loc_texts.append(\" \".join(loc_parts).strip())\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    encoded = {}\n",
    "    for name in [\"genres\", \"actors\", \"directors\", \"countries\", \"loc1\", \"loc2\", \"loc3\"]:\n",
    "        raw = encode_multihot(feat_lists[name], min_count=min_count)\n",
    "        encoded[name] = scaler.fit_transform(raw)\n",
    "    encoded[\"locations\"] = scaler.fit_transform(encode_tfidf(loc_texts, max_features=10000))\n",
    "    encoded[\"years\"] = encode_years(feat_lists[\"years\"])\n",
    "\n",
    "    tag_stats = None\n",
    "    if tag_mode != \"no_tags\":\n",
    "        if not dict_tags:\n",
    "            encoded[\"tags\"] = np.zeros((n_items, 1), dtype=np.float32)\n",
    "            tag_stats = {\"mode\": tag_mode, \"n_entries\": 0, \"n_unique_tags\": 0,\n",
    "                         \"n_items_with_tags\": 0, \"avg_tags_per_item\": 0.0, \"n_features\": 1}\n",
    "        else:\n",
    "            tag_lists = [dict_tags.get(str(idx2item[i]), []) for i in range(n_items)]\n",
    "            if shuffle_tags:\n",
    "                rng = np.random.RandomState(999)\n",
    "                rng.shuffle(tag_lists)\n",
    "            n_tag_entries = sum(len(t) for t in tag_lists)\n",
    "            n_items_with_tags = sum(1 for t in tag_lists if len(t) > 0)\n",
    "            unique_tags = set(t for tl in tag_lists for t in tl)\n",
    "            raw_tags = encode_multihot(tag_lists, min_count=min_count)\n",
    "            encoded[\"tags\"] = scaler.fit_transform(raw_tags)\n",
    "            print(f\"  Tags ({tag_mode}{'|SHUFFLED' if shuffle_tags else ''}): \"\n",
    "                  f\"{n_tag_entries} entries, {len(unique_tags)} unique, \"\n",
    "                  f\"{n_items_with_tags} items, {encoded['tags'].shape[1]} features\")\n",
    "            tag_stats = {\"mode\": tag_mode, \"n_entries\": n_tag_entries,\n",
    "                         \"n_unique_tags\": len(unique_tags),\n",
    "                         \"n_items_with_tags\": n_items_with_tags,\n",
    "                         \"n_features\": encoded[\"tags\"].shape[1]}\n",
    "\n",
    "    if tag_stats:\n",
    "        encoded[\"_tag_stats\"] = tag_stats\n",
    "    return encoded\n",
    "\n",
    "print(\"Feature encoding functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Similarity, Splits, MARec Core"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def smoothed_cosine_similarity(enc, shrinkage=20.0):\n",
    "    sim_num = enc @ enc.T\n",
    "    norms = np.linalg.norm(enc, axis=1)\n",
    "    sim_den = np.outer(norms, norms) + shrinkage\n",
    "    sim = sim_num / sim_den\n",
    "    np.fill_diagonal(sim, 0.0)\n",
    "    return sim\n",
    "\n",
    "def year_similarity(enc):\n",
    "    dist = euclidean_distances(enc)\n",
    "    max_dist = dist.max()\n",
    "    sim = 1.0 - dist / (max_dist + 1e-10) if max_dist > 0 else np.zeros_like(dist)\n",
    "    np.fill_diagonal(sim, 0.0)\n",
    "    return sim\n",
    "\n",
    "def build_similarity_matrices(encoded_features, shrinkage=20.0,\n",
    "                               per_feature_shrinkage=None, cross_pairs=None):\n",
    "    S = {}\n",
    "    for name, enc in encoded_features.items():\n",
    "        if name.startswith(\"_\"):\n",
    "            continue\n",
    "        if name == \"years\":\n",
    "            S[name] = year_similarity(enc)\n",
    "        else:\n",
    "            delta = (per_feature_shrinkage or {}).get(name, shrinkage)\n",
    "            S[name] = smoothed_cosine_similarity(enc, shrinkage=delta)\n",
    "    if cross_pairs:\n",
    "        for a, b in cross_pairs:\n",
    "            if a in S and b in S:\n",
    "                S[f\"{a}_x_{b}\"] = S[a] * S[b]\n",
    "    return S\n",
    "\n",
    "def create_cold_split_sparse(URM, cold_frac=0.20, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    n_u, n_i = URM.shape\n",
    "    n_cold = int(n_i * cold_frac)\n",
    "    cold_items = np.sort(rng.choice(n_i, n_cold, replace=False))\n",
    "    cold_set = set(cold_items)\n",
    "    coo = URM.tocoo()\n",
    "    train_mask = np.array([c not in cold_set for c in coo.col])\n",
    "    test_mask = ~train_mask\n",
    "    train = sp.csr_matrix((coo.data[train_mask], (coo.row[train_mask], coo.col[train_mask])),\n",
    "                          shape=(n_u, n_i))\n",
    "    test = sp.csr_matrix((coo.data[test_mask], (coo.row[test_mask], coo.col[test_mask])),\n",
    "                         shape=(n_u, n_i))\n",
    "    return train, test, cold_items\n",
    "\n",
    "def generate_splits(URM, n_splits=10, cold_frac=0.20, base_seed=42):\n",
    "    return [{\"train\": t, \"test\": te, \"cold_items\": ci, \"seed\": base_seed + i}\n",
    "            for i, (t, te, ci) in enumerate(\n",
    "                [create_cold_split_sparse(URM, cold_frac, base_seed + i)\n",
    "                 for i in range(n_splits)])]\n",
    "\n",
    "def compute_dr(X, beta, percentile=10):\n",
    "    v = np.asarray(X.sum(axis=0)).ravel() if sp.issparse(X) else X.sum(axis=0)\n",
    "    p = max(np.percentile(v, percentile), 1.0)\n",
    "    k = beta / p\n",
    "    return np.where(v <= p, k * (p - v), 0.0)\n",
    "\n",
    "def ease_aligned(X, Xtilde, lambda1=1.0, beta=1.0, alpha=1.0,\n",
    "                 dr_percentile=10, XtX=None):\n",
    "    n = X.shape[1]\n",
    "    dr = compute_dr(X, beta, percentile=dr_percentile)\n",
    "    Xt_dr = (alpha * Xtilde) * dr[np.newaxis, :]\n",
    "    XtXt_IR = X.T @ Xt_dr\n",
    "    if XtX is None:\n",
    "        XtX = X.T @ X\n",
    "    P = np.linalg.inv(XtX + lambda1 * np.eye(n) + XtXt_IR)\n",
    "    Bt = P @ (XtX + XtXt_IR)\n",
    "    gamma = np.diag(Bt) / np.diag(P)\n",
    "    B = Bt - P @ np.diag(gamma)\n",
    "    return B\n",
    "\n",
    "print(\"Similarity, splits, and MARec core defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def _argpartition_topk(arr, k, axis=1):\n",
    "    if _USE_BN:\n",
    "        return bn.argpartition(-arr, k, axis=axis)\n",
    "    return np.argpartition(-arr, k, axis=axis)\n",
    "\n",
    "def hit_rate_at_k(X_pred, heldout, k):\n",
    "    n = X_pred.shape[0]\n",
    "    if k >= X_pred.shape[1]:\n",
    "        k = X_pred.shape[1] - 1\n",
    "    idx = _argpartition_topk(X_pred, k)\n",
    "    pred_bin = np.zeros_like(X_pred, dtype=bool)\n",
    "    pred_bin[np.arange(n)[:, np.newaxis], idx[:, :k]] = True\n",
    "    true_bin = (heldout > 0).toarray() if sp.issparse(heldout) else (heldout > 0)\n",
    "    hits = np.logical_and(true_bin, pred_bin).sum(axis=1).astype(np.float64)\n",
    "    n_rel = true_bin.sum(axis=1).astype(np.float64)\n",
    "    denom = np.minimum(k, n_rel)\n",
    "    return hits / np.maximum(denom, 1.0)\n",
    "\n",
    "def ndcg_at_k(X_pred, heldout, k):\n",
    "    n = X_pred.shape[0]\n",
    "    if k >= X_pred.shape[1]:\n",
    "        k = X_pred.shape[1] - 1\n",
    "    idx_part = _argpartition_topk(X_pred, k)\n",
    "    topk_part = X_pred[np.arange(n)[:, np.newaxis], idx_part[:, :k]]\n",
    "    idx_sort = np.argsort(-topk_part, axis=1)\n",
    "    idx_topk = idx_part[np.arange(n)[:, np.newaxis], idx_sort]\n",
    "    tp = 1.0 / np.log2(np.arange(2, k + 2))\n",
    "    heldout_arr = heldout.toarray() if sp.issparse(heldout) else heldout\n",
    "    DCG = (heldout_arr[np.arange(n)[:, np.newaxis], idx_topk] * tp).sum(axis=1)\n",
    "    n_rel = (heldout_arr > 0).sum(axis=1)\n",
    "    IDCG = np.array([tp[:min(int(nr), k)].sum() for nr in n_rel])\n",
    "    return DCG / np.maximum(IDCG, 1e-10)\n",
    "\n",
    "def evaluate(pred, test_data, train_data, ks=(10, 25), cold_items=None):\n",
    "    pred = pred.copy()\n",
    "    n_items = pred.shape[1]\n",
    "    train_coo = train_data.tocoo()\n",
    "    pred[train_coo.row, train_coo.col] = -np.inf\n",
    "    if cold_items is not None:\n",
    "        warm_items = np.setdiff1d(np.arange(n_items), cold_items)\n",
    "        pred[:, warm_items] = -np.inf\n",
    "    test_users = np.where(np.asarray(test_data.sum(axis=1)).ravel() > 0)[0]\n",
    "    if len(test_users) == 0:\n",
    "        return {f\"{m}@{k}\": 0.0 for k in ks for m in (\"hr\", \"ndcg\")}\n",
    "    p, t = pred[test_users], test_data[test_users]\n",
    "    results = {}\n",
    "    for k in ks:\n",
    "        if k >= p.shape[1]:\n",
    "            continue\n",
    "        results[f\"hr@{k}\"] = float(np.nanmean(hit_rate_at_k(p, t, k)))\n",
    "        results[f\"ndcg@{k}\"] = float(np.nanmean(ndcg_at_k(p, t, k)))\n",
    "    return results\n",
    "\n",
    "def paired_ttest(scores_a, scores_b):\n",
    "    from scipy import stats\n",
    "    t_stat, p_value = stats.ttest_rel(scores_a, scores_b)\n",
    "    return float(t_stat), float(p_value)\n",
    "\n",
    "print(\"Evaluation metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def run_single_prediction(train_csr, feat_names, ds, tag_mode, hp,\n",
    "                          min_count=2, shrinkage=20.0, shuffle_tags=False):\n",
    "    needs_tags = any(\"tags\" in f for f in feat_names)\n",
    "    actual_tag_mode = tag_mode if needs_tags else \"no_tags\"\n",
    "    encoded = build_feature_matrices(\n",
    "        ds[\"dfs\"], ds[\"item2idx\"], ds[\"idx2item\"], ds[\"n_items\"],\n",
    "        tag_mode=actual_tag_mode,\n",
    "        train_csr=train_csr if actual_tag_mode not in (\"no_tags\", \"tags_full\") else None,\n",
    "        user2idx=ds[\"user2idx\"] if actual_tag_mode not in (\"no_tags\", \"tags_full\") else None,\n",
    "        min_count=min_count, shuffle_tags=shuffle_tags)\n",
    "    tag_stats = encoded.pop(\"_tag_stats\", None)\n",
    "    if needs_tags:\n",
    "        assert \"tags\" in encoded, f\"Tags required but not built! tag_mode={actual_tag_mode}\"\n",
    "    S = build_similarity_matrices(encoded, shrinkage=shrinkage)\n",
    "    sim_list = []\n",
    "    for f in feat_names:\n",
    "        if f in S:\n",
    "            sim_list.append(S[f])\n",
    "        else:\n",
    "            for sname in S:\n",
    "                if f in sname:\n",
    "                    sim_list.append(S[sname])\n",
    "                    break\n",
    "    X = train_csr.toarray().astype(np.float64)\n",
    "    XtX = X.T @ X\n",
    "    XG = [X @ G for G in sim_list]\n",
    "    y = X.ravel()\n",
    "    Xr = np.column_stack([xg.ravel() for xg in XG])\n",
    "    w = np.where(y > 0, 1.0, 0.2)\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(Xr, y, sample_weight=w)\n",
    "    coefs = reg.coef_\n",
    "    Xtilde = sum(c * xg for c, xg in zip(coefs, XG))\n",
    "    B = ease_aligned(X, Xtilde, lambda1=hp[\"lambda1\"], beta=hp[\"beta\"],\n",
    "                     alpha=hp[\"alpha\"], XtX=XtX)\n",
    "    return X @ B, tag_stats\n",
    "\n",
    "def run_experiment(ds, splits, feat_names, tag_mode, config_name,\n",
    "                   hp_grid=None, n_splits=10, eval_ks=(10, 25),\n",
    "                   min_count=2, shrinkage=20.0, tune_splits=None,\n",
    "                   shuffle_tags=False):\n",
    "    if hp_grid is None:\n",
    "        hp_grid = {\"lambda1\": [0.1, 1, 10, 100],\n",
    "                   \"beta\": [1, 10, 100, 500],\n",
    "                   \"alpha\": [0.1, 1, 10, 100]}\n",
    "    if tune_splits is None:\n",
    "        tune_splits = [0, 1, 2]\n",
    "    n_splits = min(n_splits, len(splits))\n",
    "    print(f\"\\n{'='*65}\")\n",
    "    print(f\"  Experiment: {config_name}\")\n",
    "    print(f\"  Features: {feat_names}\")\n",
    "    print(f\"  Tag mode: {tag_mode}{'  [SHUFFLED CONTROL]' if shuffle_tags else ''}\")\n",
    "    print(f\"  Splits: {n_splits}, Tune on: {tune_splits}\")\n",
    "    print(f\"{'='*65}\")\n",
    "    # HP Tuning\n",
    "    t0 = time.time()\n",
    "    best_score, best_hp = -1, {\"lambda1\": 1.0, \"beta\": 100.0, \"alpha\": 1.0}\n",
    "    hp_combos = list(iterproduct(hp_grid[\"lambda1\"], hp_grid[\"beta\"], hp_grid[\"alpha\"]))\n",
    "    print(f\"  HP grid: {len(hp_combos)} combos\")\n",
    "    for l1, beta, alpha in hp_combos:\n",
    "        scores = []\n",
    "        for si in tune_splits:\n",
    "            if si >= len(splits):\n",
    "                continue\n",
    "            pred, _ = run_single_prediction(\n",
    "                splits[si][\"train\"], feat_names, ds, tag_mode,\n",
    "                {\"lambda1\": l1, \"beta\": beta, \"alpha\": alpha},\n",
    "                min_count=min_count, shrinkage=shrinkage, shuffle_tags=shuffle_tags)\n",
    "            m = evaluate(pred, splits[si][\"test\"], splits[si][\"train\"],\n",
    "                         ks=[eval_ks[0]], cold_items=splits[si][\"cold_items\"])\n",
    "            scores.append(m.get(f\"hr@{eval_ks[0]}\", 0))\n",
    "        avg = np.mean(scores) if scores else 0\n",
    "        if avg > best_score:\n",
    "            best_score = avg\n",
    "            best_hp = {\"lambda1\": l1, \"beta\": beta, \"alpha\": alpha}\n",
    "    print(f\"  Best HP: l1={best_hp['lambda1']}, beta={best_hp['beta']}, \"\n",
    "          f\"alpha={best_hp['alpha']} (tune={best_score:.4f}, {time.time()-t0:.0f}s)\")\n",
    "    # Full Evaluation\n",
    "    t1 = time.time()\n",
    "    per_split = []\n",
    "    tag_stats = None\n",
    "    for si in range(n_splits):\n",
    "        pred, ts = run_single_prediction(\n",
    "            splits[si][\"train\"], feat_names, ds, tag_mode, best_hp,\n",
    "            min_count=min_count, shrinkage=shrinkage, shuffle_tags=shuffle_tags)\n",
    "        if ts and tag_stats is None:\n",
    "            tag_stats = ts\n",
    "        m = evaluate(pred, splits[si][\"test\"], splits[si][\"train\"],\n",
    "                     ks=eval_ks, cold_items=splits[si][\"cold_items\"])\n",
    "        per_split.append(m)\n",
    "        if si == 0:\n",
    "            print(f\"  Split 0: {m}\")\n",
    "    avg = {k: np.mean([m[k] for m in per_split]) for k in per_split[0]}\n",
    "    std = {k: np.std([m[k] for m in per_split]) for k in per_split[0]}\n",
    "    print(f\"\\n  Results ({n_splits} splits, {time.time()-t1:.0f}s):\")\n",
    "    for k in sorted(avg.keys()):\n",
    "        print(f\"     {k}: {avg[k]:.4f} +/- {std[k]:.4f}\")\n",
    "    return {\"config_name\": config_name, \"feature_names\": feat_names,\n",
    "            \"tag_mode\": tag_mode, \"best_hp\": best_hp,\n",
    "            \"avg_metrics\": avg, \"std_metrics\": std,\n",
    "            \"per_split_metrics\": per_split, \"tag_stats\": tag_stats,\n",
    "            \"shuffle_tags\": shuffle_tags}\n",
    "\n",
    "print(\"Experiment runner defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Data & Generate Splits"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\"hetrec_data\", threshold=3.0)\n",
    "\n",
    "N_SPLITS = 10\n",
    "print(f\"\\nGenerating {N_SPLITS} cold-start splits...\")\n",
    "splits = generate_splits(ds[\"URM\"], n_splits=N_SPLITS, cold_frac=0.20, base_seed=42)\n",
    "print(f\"{N_SPLITS} splits generated\")\n",
    "\n",
    "# Verify split integrity\n",
    "s0 = splits[0]\n",
    "cold_set = set(s0[\"cold_items\"])\n",
    "train_cols = set(s0[\"train\"].tocoo().col)\n",
    "test_cols = set(s0[\"test\"].tocoo().col)\n",
    "assert len(train_cols & cold_set) == 0, \"LEAK: cold items in train!\"\n",
    "assert test_cols.issubset(cold_set), \"LEAK: warm items in test!\"\n",
    "print(f\"Split integrity verified (cold items: {len(cold_set)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tag Statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"  TAG STATISTICS\")\n",
    "print(\"=\"*65)\n",
    "tag_df = ds[\"dfs\"][\"tags\"]\n",
    "user_tag_df = ds[\"dfs\"][\"user_tags\"]\n",
    "print(f\"  movie_tags.dat: {len(tag_df)} rows, columns: {tag_df.columns.tolist()}\")\n",
    "print(f\"  user_taggedmovies: {len(user_tag_df)} rows, columns: {user_tag_df.columns.tolist()}\")\n",
    "unique_tags_agg = tag_df[\"tagID\"].nunique() if \"tagID\" in tag_df.columns else 0\n",
    "unique_items_tagged = tag_df[\"movieID\"].nunique() if \"movieID\" in tag_df.columns else 0\n",
    "unique_users_tagging = user_tag_df[\"userID\"].nunique() if \"userID\" in user_tag_df.columns else 0\n",
    "print(f\"  Unique tag IDs: {unique_tags_agg}\")\n",
    "print(f\"  Items with tags (aggregated): {unique_items_tagged}\")\n",
    "print(f\"  Users who tagged: {unique_users_tagging}\")\n",
    "print(f\"  Items in dataset: {ds['n_items']}\")\n",
    "print(f\"  Tag coverage: {unique_items_tagged/ds['n_items']*100:.1f}% of items have tags\")\n",
    "\n",
    "cold_items_0 = splits[0][\"cold_items\"]\n",
    "cold_item_ids = set(str(ds[\"idx2item\"][i]) for i in cold_items_0)\n",
    "tagged_items = set(tag_df[\"movieID\"].astype(str).unique())\n",
    "cold_with_tags = cold_item_ids & tagged_items\n",
    "print(f\"\\n  Cold items (split 0): {len(cold_items_0)}\")\n",
    "print(f\"  Cold items with tags: {len(cold_with_tags)} ({len(cold_with_tags)/len(cold_items_0)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment 1: Baseline Reproduction (10 splits)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "HP_GRID = {\n",
    "    \"lambda1\": [0.1, 1, 10, 100],\n",
    "    \"beta\": [1, 10, 100, 500],\n",
    "    \"alpha\": [0.1, 1, 10, 100],\n",
    "}\n",
    "TUNE_SPLITS = [0, 1, 2]\n",
    "\n",
    "results = {}\n",
    "\n",
    "results[\"top3_no_tags\"] = run_experiment(\n",
    "    ds, splits, feat_names=[\"actors\", \"directors\", \"genres\"],\n",
    "    tag_mode=\"no_tags\", config_name=\"top3_no_tags\",\n",
    "    hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment 2: Tag Modes (10 splits)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tags train users (leakage-safe)\n",
    "results[\"top3_tags_safe\"] = run_experiment(\n",
    "    ds, splits, feat_names=[\"actors\", \"directors\", \"genres\", \"tags\"],\n",
    "    tag_mode=\"tags_train_users\", config_name=\"top3_tags_safe\",\n",
    "    hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tags full (upper bound)\n",
    "results[\"top3_tags_full\"] = run_experiment(\n",
    "    ds, splits, feat_names=[\"actors\", \"directors\", \"genres\", \"tags\"],\n",
    "    tag_mode=\"tags_full\", config_name=\"top3_tags_full\",\n",
    "    hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Experiment 3: Feature Ablation (10 splits)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Individual features\n",
    "for feat_name in [\"actors\", \"directors\", \"genres\"]:\n",
    "    results[f\"single_{feat_name}\"] = run_experiment(\n",
    "        ds, splits, feat_names=[feat_name],\n",
    "        tag_mode=\"no_tags\", config_name=f\"single_{feat_name}\",\n",
    "        hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tags only (leakage-safe)\n",
    "results[\"single_tags_safe\"] = run_experiment(\n",
    "    ds, splits, feat_names=[\"tags\"],\n",
    "    tag_mode=\"tags_train_users\", config_name=\"single_tags_safe\",\n",
    "    hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Top3 + countries\n",
    "results[\"top3_countries\"] = run_experiment(\n",
    "    ds, splits, feat_names=[\"actors\", \"directors\", \"genres\", \"countries\"],\n",
    "    tag_mode=\"no_tags\", config_name=\"top3_countries\",\n",
    "    hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS)\n",
    "\n",
    "# Top3 + locations\n",
    "results[\"top3_locations\"] = run_experiment(\n",
    "    ds, splits, feat_names=[\"actors\", \"directors\", \"genres\", \"locations\"],\n",
    "    tag_mode=\"no_tags\", config_name=\"top3_locations\",\n",
    "    hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS)\n",
    "\n",
    "# Top3 + years\n",
    "results[\"top3_years\"] = run_experiment(\n",
    "    ds, splits, feat_names=[\"actors\", \"directors\", \"genres\", \"years\"],\n",
    "    tag_mode=\"no_tags\", config_name=\"top3_years\",\n",
    "    hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Full 9-feature baseline\n",
    "results[\"base9_no_tags\"] = run_experiment(\n",
    "    ds, splits,\n",
    "    feat_names=[\"genres\", \"actors\", \"directors\", \"countries\",\n",
    "                \"loc1\", \"loc2\", \"loc3\", \"years\", \"locations\"],\n",
    "    tag_mode=\"no_tags\", config_name=\"base9_no_tags\",\n",
    "    hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS)\n",
    "\n",
    "# Full 9-feature + tags\n",
    "results[\"base9_tags_safe\"] = run_experiment(\n",
    "    ds, splits,\n",
    "    feat_names=[\"genres\", \"actors\", \"directors\", \"countries\",\n",
    "                \"loc1\", \"loc2\", \"loc3\", \"years\", \"locations\", \"tags\"],\n",
    "    tag_mode=\"tags_train_users\", config_name=\"base9_tags_safe\",\n",
    "    hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Control: Random Tag Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# If tags carry content signal, shuffling should collapse performance\n",
    "results[\"tags_shuffled\"] = run_experiment(\n",
    "    ds, splits, feat_names=[\"actors\", \"directors\", \"genres\", \"tags\"],\n",
    "    tag_mode=\"tags_train_users\", config_name=\"tags_shuffled\",\n",
    "    hp_grid=HP_GRID, n_splits=N_SPLITS, tune_splits=TUNE_SPLITS,\n",
    "    shuffle_tags=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  COMPREHENSIVE RESULTS TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "display_order = [\n",
    "    \"single_actors\", \"single_directors\", \"single_genres\",\n",
    "    \"top3_no_tags\", \"top3_countries\", \"top3_locations\", \"top3_years\",\n",
    "    \"base9_no_tags\",\n",
    "    \"single_tags_safe\", \"top3_tags_safe\", \"base9_tags_safe\",\n",
    "    \"top3_tags_full\",\n",
    "    \"tags_shuffled\",\n",
    "]\n",
    "\n",
    "rows = []\n",
    "baseline_hr = results.get(\"top3_no_tags\", {}).get(\"avg_metrics\", {}).get(\"hr@10\", 0)\n",
    "\n",
    "for name in display_order:\n",
    "    if name not in results:\n",
    "        continue\n",
    "    r = results[name]\n",
    "    avg = r[\"avg_metrics\"]\n",
    "    std = r[\"std_metrics\"]\n",
    "    delta = ((avg.get(\"hr@10\", 0) / baseline_hr) - 1) * 100 if baseline_hr > 0 else 0\n",
    "    rows.append({\n",
    "        \"Config\": name,\n",
    "        \"Features\": \", \".join(r[\"feature_names\"]),\n",
    "        \"Tag Mode\": r[\"tag_mode\"],\n",
    "        \"hr@10\": f\"{avg.get('hr@10', 0):.4f} +/- {std.get('hr@10', 0):.4f}\",\n",
    "        \"ndcg@10\": f\"{avg.get('ndcg@10', 0):.4f} +/- {std.get('ndcg@10', 0):.4f}\",\n",
    "        \"Delta hr@10\": f\"{delta:+.1f}%\",\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(rows)\n",
    "try:\n",
    "    from tabulate import tabulate\n",
    "    print(tabulate(df_results, headers=\"keys\", tablefmt=\"grid\", showindex=False))\n",
    "except ImportError:\n",
    "    print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*65)\n",
    "print(\"  PAIRED STATISTICAL TESTS vs top3_no_tags (baseline)\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "baseline_per_split = [m.get(\"hr@10\", 0) for m in results[\"top3_no_tags\"][\"per_split_metrics\"]]\n",
    "sig_rows = []\n",
    "for name in display_order:\n",
    "    if name not in results or name == \"top3_no_tags\":\n",
    "        continue\n",
    "    other_per_split = [m.get(\"hr@10\", 0) for m in results[name][\"per_split_metrics\"]]\n",
    "    if len(baseline_per_split) > 1 and len(other_per_split) > 1:\n",
    "        t_stat, p_val = paired_ttest(other_per_split, baseline_per_split)\n",
    "        sig = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "        sig_rows.append({\n",
    "            \"Config\": name,\n",
    "            \"Mean Delta hr@10\": f\"{np.mean(other_per_split) - np.mean(baseline_per_split):+.4f}\",\n",
    "            \"t-statistic\": f\"{t_stat:.3f}\",\n",
    "            \"p-value\": f\"{p_val:.6f}\",\n",
    "            \"Significance\": sig,\n",
    "        })\n",
    "\n",
    "df_sig = pd.DataFrame(sig_rows)\n",
    "try:\n",
    "    from tabulate import tabulate\n",
    "    print(tabulate(df_sig, headers=\"keys\", tablefmt=\"grid\", showindex=False))\n",
    "except ImportError:\n",
    "    print(df_sig.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bar chart: hr@10 across all configs\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "config_names = []\n",
    "hr_means = []\n",
    "hr_stds = []\n",
    "colors = []\n",
    "color_map = {\"no_tags\": \"#4A90D9\", \"tags_train_users\": \"#50C878\", \"tags_full\": \"#FFB347\"}\n",
    "\n",
    "for name in display_order:\n",
    "    if name not in results:\n",
    "        continue\n",
    "    r = results[name]\n",
    "    config_names.append(name)\n",
    "    hr_means.append(r[\"avg_metrics\"].get(\"hr@10\", 0))\n",
    "    hr_stds.append(r[\"std_metrics\"].get(\"hr@10\", 0))\n",
    "    if r.get(\"shuffle_tags\"):\n",
    "        colors.append(\"#FF6B6B\")\n",
    "    else:\n",
    "        colors.append(color_map.get(r[\"tag_mode\"], \"#4A90D9\"))\n",
    "\n",
    "bars = ax.bar(range(len(config_names)), hr_means, yerr=hr_stds,\n",
    "              color=colors, edgecolor=\"black\", linewidth=0.5, capsize=3)\n",
    "if baseline_hr > 0:\n",
    "    ax.axhline(y=baseline_hr, color=\"gray\", linestyle=\"--\", alpha=0.7, label=f\"Baseline ({baseline_hr:.3f})\")\n",
    "ax.set_xticks(range(len(config_names)))\n",
    "ax.set_xticklabels(config_names, rotation=45, ha=\"right\", fontsize=9)\n",
    "ax.set_ylabel(\"HR@10\")\n",
    "ax.set_title(\"MARec Cold-Start Performance: Feature Ablation & Tag Modes (10 splits)\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0, max(hr_means) * 1.15)\n",
    "for bar, val in zip(bars, hr_means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "            f\"{val:.3f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hr10_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved hr10_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Per-split variance plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for name in [\"top3_no_tags\", \"top3_tags_safe\", \"top3_tags_full\"]:\n",
    "    if name not in results:\n",
    "        continue\n",
    "    r = results[name]\n",
    "    vals = [m.get(\"hr@10\", 0) for m in r[\"per_split_metrics\"]]\n",
    "    ax.plot(range(len(vals)), vals, \"o-\", label=name, markersize=6)\n",
    "ax.set_xlabel(\"Split Index\")\n",
    "ax.set_ylabel(\"HR@10\")\n",
    "ax.set_title(\"Per-Split HR@10 Variance (cold-start stability)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"per_split_variance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved per_split_variance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_dir = Path(\"results/full_experiments\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_rows = []\n",
    "for name, r in results.items():\n",
    "    row = {\"config\": name, \"tag_mode\": r[\"tag_mode\"],\n",
    "           \"features\": \"|\".join(r[\"feature_names\"]),\n",
    "           \"shuffled\": r.get(\"shuffle_tags\", False)}\n",
    "    row.update(r[\"best_hp\"])\n",
    "    row.update({k: v for k, v in r[\"avg_metrics\"].items()})\n",
    "    row.update({f\"{k}_std\": v for k, v in r[\"std_metrics\"].items()})\n",
    "    all_rows.append(row)\n",
    "df_all = pd.DataFrame(all_rows)\n",
    "df_all.to_csv(output_dir / \"all_results.csv\", index=False)\n",
    "pd.DataFrame([{\"config\": n, \"split\": si, **m}\n",
    "    for n, r in results.items()\n",
    "    for si, m in enumerate(r[\"per_split_metrics\"])\n",
    "]).to_csv(output_dir / \"per_split_metrics.csv\", index=False)\n",
    "\n",
    "json_results = {}\n",
    "for name, r in results.items():\n",
    "    json_results[name] = {\n",
    "        \"config_name\": name, \"feature_names\": r[\"feature_names\"],\n",
    "        \"tag_mode\": r[\"tag_mode\"], \"best_hp\": r[\"best_hp\"],\n",
    "        \"avg_metrics\": r[\"avg_metrics\"], \"std_metrics\": r[\"std_metrics\"],\n",
    "        \"shuffle_tags\": r.get(\"shuffle_tags\", False)}\n",
    "with open(output_dir / \"all_results.json\", \"w\") as f:\n",
    "    json.dump(json_results, f, indent=2, default=float)\n",
    "\n",
    "print(f\"All results saved to {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"  FINAL EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n  Dataset: HetRec 2011 MovieLens\")\n",
    "print(f\"  Users: {ds['n_users']}, Items: {ds['n_items']}, Interactions: {ds['URM'].nnz}\")\n",
    "print(f\"  Splits: {N_SPLITS}, Cold fraction: 20%\")\n",
    "\n",
    "print(f\"\\n  Key Results:\")\n",
    "for name in [\"top3_no_tags\", \"top3_tags_safe\", \"top3_tags_full\", \"tags_shuffled\"]:\n",
    "    if name in results:\n",
    "        r = results[name]\n",
    "        hr = r[\"avg_metrics\"].get(\"hr@10\", 0)\n",
    "        std = r[\"std_metrics\"].get(\"hr@10\", 0)\n",
    "        delta = ((hr / baseline_hr) - 1) * 100 if baseline_hr > 0 else 0\n",
    "        label = \"CONTROL\" if r.get(\"shuffle_tags\") else \"OK\"\n",
    "        print(f\"    [{label}] {name:25s} hr@10={hr:.4f}+/-{std:.4f}  ({delta:+.1f}%)\")\n",
    "\n",
    "print(f\"\\n  Scientific Controls:\")\n",
    "if \"tags_shuffled\" in results:\n",
    "    shuf_hr = results[\"tags_shuffled\"][\"avg_metrics\"].get(\"hr@10\", 0)\n",
    "    safe_hr = results[\"top3_tags_safe\"][\"avg_metrics\"].get(\"hr@10\", 0) if \"top3_tags_safe\" in results else 0\n",
    "    if shuf_hr < baseline_hr * 1.1:\n",
    "        print(f\"    Shuffled tags collapsed to ~baseline -> tags carry CONTENT signal\")\n",
    "    elif shuf_hr < safe_hr * 0.9:\n",
    "        print(f\"    Shuffled tags much worse than real tags -> content signal confirmed\")\n",
    "    else:\n",
    "        print(f\"    WARNING: Shuffled tags still show improvement -> investigate density effect\")\n",
    "\n",
    "print(f\"\\n  All {len(results)} experiments completed successfully!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ]
}