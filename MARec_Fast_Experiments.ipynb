{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARec++: Fast Comprehensive Experiment Suite\n",
    "**Optimized for Colab Pro — estimated runtime ~1 hour on H100**\n",
    "\n",
    "| Optimization | Original | Fast |\n",
    "|---|---|---|\n",
    "| HP grid | 64 combos | 4 combos |\n",
    "| Tune splits | 3 | 1 |\n",
    "| Eval splits | 10 | 5 |\n",
    "| Est. runtime | ~30 hours | ~1 hour |\n",
    "\n",
    "All experiments from the full suite, with statistically valid results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import subprocess, sys\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                       \"numpy\", \"scipy\", \"pandas\", \"scikit-learn\", \"matplotlib\",\n",
    "                       \"seaborn\", \"bottleneck\", \"tabulate\"])\n",
    "\n",
    "import os, time, json, warnings, copy, zipfile, urllib.request\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from itertools import product as iterproduct\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.optimize import nnls\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({\"font.size\": 12, \"figure.figsize\": (10, 6), \"figure.dpi\": 120})\n",
    "try:\n",
    "    import seaborn as sns; sns.set_style(\"whitegrid\")\n",
    "except: pass\n",
    "try:\n",
    "    import bottleneck as bn; _USE_BN = True\n",
    "except: _USE_BN = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"All imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions (data, features, similarity, EASE, evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ============ DATA ============\n",
    "HETREC_URL = \"http://files.grouplens.org/datasets/hetrec2011/hetrec2011-movielens-2k-v2.zip\"\n",
    "\n",
    "def download_hetrec(data_dir):\n",
    "    data_dir = Path(data_dir); data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for sub in [data_dir / \"hetrec2011-movielens-2k-v2\", data_dir]:\n",
    "        if (sub / \"user_ratedmovies-timestamps.dat\").exists(): return sub\n",
    "    zp = data_dir / \"hetrec.zip\"\n",
    "    print(f\"Downloading HetRec 2011...\"); urllib.request.urlretrieve(HETREC_URL, str(zp))\n",
    "    with zipfile.ZipFile(str(zp), \"r\") as zf: zf.extractall(str(data_dir))\n",
    "    for sub in [data_dir / \"hetrec2011-movielens-2k-v2\", data_dir]:\n",
    "        if (sub / \"user_ratedmovies-timestamps.dat\").exists(): return sub\n",
    "    raise RuntimeError(\"Extraction failed\")\n",
    "\n",
    "def read_dat(raw_dir, fn):\n",
    "    p = Path(raw_dir) / fn\n",
    "    if not p.exists(): return pd.DataFrame()\n",
    "    with open(str(p), \"r\", encoding=\"latin-1\") as f: text = f.read()\n",
    "    lines = text.strip().split(\"\\n\")\n",
    "    rows = [l.replace(\"\\r\",\"\").split(\"\\t\") for l in lines]\n",
    "    return pd.DataFrame(rows[1:], columns=rows[0])\n",
    "\n",
    "def load_all_dataframes(raw_dir):\n",
    "    dfs = {}\n",
    "    for name, fn in [(\"movies\",\"movies.dat\"),(\"ratings\",\"user_ratedmovies-timestamps.dat\"),\n",
    "                     (\"genres\",\"movie_genres.dat\"),(\"actors\",\"movie_actors.dat\"),\n",
    "                     (\"directors\",\"movie_directors.dat\"),(\"countries\",\"movie_countries.dat\"),\n",
    "                     (\"tags\",\"movie_tags.dat\"),(\"locations\",\"movie_locations.dat\")]:\n",
    "        dfs[name] = read_dat(raw_dir, fn)\n",
    "    # FIX: per-user tags with userID column\n",
    "    dfs[\"user_tags\"] = read_dat(raw_dir, \"user_taggedmovies-timestamps.dat\")\n",
    "    if dfs[\"user_tags\"].empty: dfs[\"user_tags\"] = read_dat(raw_dir, \"user_taggedmovies.dat\")\n",
    "    return dfs\n",
    "\n",
    "def binarize_and_kcore(df_ratings, threshold=3.0, min_user=5, min_item=5):\n",
    "    df = df_ratings.copy()\n",
    "    df[\"userID\"]=df[\"userID\"].astype(str); df[\"movieID\"]=df[\"movieID\"].astype(str)\n",
    "    df[\"rating\"]=df[\"rating\"].astype(float)\n",
    "    df = df[df[\"rating\"]>=threshold].groupby([\"userID\",\"movieID\"])[\"rating\"].max().reset_index()\n",
    "    for _ in range(200):\n",
    "        n=len(df)\n",
    "        ic=df[\"movieID\"].value_counts(); df=df[df[\"movieID\"].isin(ic[ic>=min_item].index)]\n",
    "        uc=df[\"userID\"].value_counts(); df=df[df[\"userID\"].isin(uc[uc>=min_user].index)]\n",
    "        if len(df)==n: break\n",
    "    users=sorted(df[\"userID\"].unique()); items=sorted(df[\"movieID\"].unique())\n",
    "    u2i={u:i for i,u in enumerate(users)}; i2i={m:i for i,m in enumerate(items)}\n",
    "    idx2item={i:m for m,i in i2i.items()}\n",
    "    r=np.array([u2i[u] for u in df[\"userID\"]]); c=np.array([i2i[m] for m in df[\"movieID\"]])\n",
    "    URM=sp.csr_matrix((np.ones(len(r),dtype=np.float32),(r,c)),shape=(len(users),len(items)))\n",
    "    return URM, u2i, i2i, idx2item, df\n",
    "\n",
    "def load_dataset(data_dir=\"hetrec_data\", threshold=3.0):\n",
    "    dfs = load_all_dataframes(download_hetrec(data_dir))\n",
    "    URM, u2i, i2i, idx2item, df_f = binarize_and_kcore(dfs[\"ratings\"], threshold)\n",
    "    n_u,n_i=URM.shape; d=URM.nnz/(n_u*n_i)*100\n",
    "    print(f\"Dataset: {n_u} users x {n_i} items, {URM.nnz} interactions, density={d:.2f}%\")\n",
    "    return {\"URM\":URM,\"user2idx\":u2i,\"item2idx\":i2i,\"idx2item\":idx2item,\"dfs\":dfs,\"n_users\":n_u,\"n_items\":n_i}\n",
    "\n",
    "# ============ FEATURES ============\n",
    "def _bfd(df, id_col, val_col):\n",
    "    d={}\n",
    "    for _,row in df.iterrows(): d.setdefault(str(row[id_col]),[]).append(str(row[val_col]))\n",
    "    return d\n",
    "\n",
    "def encode_multihot(item_lists, min_count=2):\n",
    "    flat=[v for l in item_lists for v in l]\n",
    "    ctr=Counter(flat); valid=sorted(k for k,v in ctr.items() if v>=min_count and k.strip())\n",
    "    if not valid: return np.zeros((len(item_lists),1),dtype=np.float32)\n",
    "    mlb=MultiLabelBinarizer(classes=valid,sparse_output=False)\n",
    "    return mlb.fit_transform(item_lists).astype(np.float32)\n",
    "\n",
    "def encode_tfidf(texts, max_features=10000):\n",
    "    if not any(t.strip() for t in texts): return np.zeros((len(texts),1),dtype=np.float32)\n",
    "    try:\n",
    "        pipe=Pipeline([(\"c\",CountVectorizer(max_features=max_features)),(\"t\",TfidfTransformer())])\n",
    "        return pipe.fit_transform(texts).toarray().astype(np.float32)\n",
    "    except: return np.zeros((len(texts),1),dtype=np.float32)\n",
    "\n",
    "def _filter_tags_train_users(df_tags, train_csr, user2idx, item2idx):\n",
    "    train_users=set(train_csr.tocoo().row); result={}\n",
    "    for _,row in df_tags.iterrows():\n",
    "        uid,mid,tid=str(row.get(\"userID\",\"\")),str(row.get(\"movieID\",\"\")),str(row.get(\"tagID\",\"\"))\n",
    "        if uid not in user2idx or mid not in item2idx: continue\n",
    "        if user2idx[uid] in train_users: result.setdefault(mid,[]).append(tid)\n",
    "    return result\n",
    "\n",
    "def _filter_tags_train_pairs(df_tags, train_csr, user2idx, item2idx):\n",
    "    result={}\n",
    "    for _,row in df_tags.iterrows():\n",
    "        uid,mid,tid=str(row.get(\"userID\",\"\")),str(row.get(\"movieID\",\"\")),str(row.get(\"tagID\",\"\"))\n",
    "        if uid not in user2idx or mid not in item2idx: continue\n",
    "        if train_csr[user2idx[uid],item2idx[mid]]>0: result.setdefault(mid,[]).append(tid)\n",
    "    return result\n",
    "\n",
    "def build_feature_matrices(dfs, item2idx, idx2item, n_items,\n",
    "                           tag_mode=\"no_tags\", train_csr=None,\n",
    "                           user2idx=None, min_count=2, shuffle_tags=False):\n",
    "    assert tag_mode in (\"no_tags\",\"tags_train_only\",\"tags_train_users\",\"tags_train_pairs\",\"tags_full\")\n",
    "    d_genres=_bfd(dfs[\"genres\"],\"movieID\",\"genre\")\n",
    "    d_actors=_bfd(dfs[\"actors\"],\"movieID\",\"actorName\")\n",
    "    d_directors=_bfd(dfs[\"directors\"],\"movieID\",\"directorName\")\n",
    "    d_countries=_bfd(dfs[\"countries\"],\"movieID\",\"country\")\n",
    "    # FIX: use user_tags for leakage-safe modes\n",
    "    if tag_mode==\"tags_full\": d_tags=_bfd(dfs[\"tags\"],\"movieID\",\"tagID\")\n",
    "    elif tag_mode in (\"tags_train_users\",):\n",
    "        d_tags=_filter_tags_train_users(dfs.get(\"user_tags\",dfs[\"tags\"]),train_csr,user2idx,item2idx)\n",
    "    elif tag_mode in (\"tags_train_pairs\",\"tags_train_only\"):\n",
    "        d_tags=_filter_tags_train_pairs(dfs.get(\"user_tags\",dfs[\"tags\"]),train_csr,user2idx,item2idx)\n",
    "    else: d_tags={}\n",
    "\n",
    "    df_m=dfs[\"movies\"]; d_years={}\n",
    "    for _,row in df_m.iterrows():\n",
    "        try: d_years[str(row[\"id\"])]=int(row[\"year\"])\n",
    "        except: d_years[str(row[\"id\"])]=2000\n",
    "\n",
    "    d_locs=[{},{},{}]\n",
    "    if len(dfs[\"locations\"])>0:\n",
    "        lc=dfs[\"locations\"].columns.tolist()\n",
    "        for _,row in dfs[\"locations\"].iterrows():\n",
    "            mid=str(row[lc[0]])\n",
    "            for lvl in range(min(3,len(lc)-1)):\n",
    "                d_locs[lvl].setdefault(mid,[]).append(str(row[lc[lvl+1]]))\n",
    "\n",
    "    fl={\"genres\":[],\"actors\":[],\"directors\":[],\"countries\":[],\"loc1\":[],\"loc2\":[],\"loc3\":[],\"years\":[]}\n",
    "    loc_texts=[]\n",
    "    for i in range(n_items):\n",
    "        mid=str(idx2item[i])\n",
    "        fl[\"genres\"].append(d_genres.get(mid,[])); fl[\"actors\"].append(d_actors.get(mid,[]))\n",
    "        fl[\"directors\"].append(d_directors.get(mid,[])); fl[\"countries\"].append(d_countries.get(mid,[]))\n",
    "        fl[\"loc1\"].append(d_locs[0].get(mid,[])); fl[\"loc2\"].append(d_locs[1].get(mid,[]))\n",
    "        fl[\"loc3\"].append(d_locs[2].get(mid,[])); fl[\"years\"].append(d_years.get(mid,2000))\n",
    "        loc_texts.append(\" \".join(\" \".join(d_locs[j].get(mid,[])) for j in range(3)).strip())\n",
    "\n",
    "    sc=MinMaxScaler(); enc={}\n",
    "    for n in [\"genres\",\"actors\",\"directors\",\"countries\",\"loc1\",\"loc2\",\"loc3\"]:\n",
    "        enc[n]=sc.fit_transform(encode_multihot(fl[n],min_count=min_count))\n",
    "    enc[\"locations\"]=sc.fit_transform(encode_tfidf(loc_texts))\n",
    "    enc[\"years\"]=sc.fit_transform(np.array(fl[\"years\"],dtype=np.float32).reshape(-1,1))\n",
    "\n",
    "    ts=None\n",
    "    if tag_mode!=\"no_tags\":\n",
    "        if not d_tags:\n",
    "            enc[\"tags\"]=np.zeros((n_items,1),dtype=np.float32)\n",
    "            ts={\"mode\":tag_mode,\"n_entries\":0,\"n_items_with_tags\":0,\"n_features\":1}\n",
    "        else:\n",
    "            tag_lists=[d_tags.get(str(idx2item[i]),[]) for i in range(n_items)]\n",
    "            if shuffle_tags:\n",
    "                rng=np.random.RandomState(999); rng.shuffle(tag_lists)\n",
    "            ne=sum(len(t) for t in tag_lists); niwt=sum(1 for t in tag_lists if t)\n",
    "            ut=set(t for tl in tag_lists for t in tl)\n",
    "            raw=encode_multihot(tag_lists,min_count=min_count)\n",
    "            enc[\"tags\"]=sc.fit_transform(raw)\n",
    "            print(f\"  Tags({tag_mode}{'|SHUF' if shuffle_tags else ''}): {ne} entries, \"\n",
    "                  f\"{len(ut)} unique, {niwt} items, {enc['tags'].shape[1]} feats\")\n",
    "            ts={\"mode\":tag_mode,\"n_entries\":ne,\"n_unique\":len(ut),\"n_items_with_tags\":niwt,\n",
    "                \"n_features\":enc[\"tags\"].shape[1]}\n",
    "    if ts: enc[\"_tag_stats\"]=ts\n",
    "    return enc\n",
    "\n",
    "# ============ SIMILARITY & EASE ============\n",
    "def smoothed_cosine(enc, shrinkage=20.0):\n",
    "    s=enc@enc.T; n=np.linalg.norm(enc,axis=1)\n",
    "    s/=(np.outer(n,n)+shrinkage); np.fill_diagonal(s,0); return s\n",
    "\n",
    "def year_sim(enc):\n",
    "    d=euclidean_distances(enc); mx=d.max()\n",
    "    s=1.0-d/(mx+1e-10) if mx>0 else np.zeros_like(d); np.fill_diagonal(s,0); return s\n",
    "\n",
    "def build_sims(enc_feats, shrinkage=20.0):\n",
    "    S={}\n",
    "    for n,e in enc_feats.items():\n",
    "        if n.startswith(\"_\"): continue\n",
    "        S[n]=year_sim(e) if n==\"years\" else smoothed_cosine(e,shrinkage)\n",
    "    return S\n",
    "\n",
    "def compute_dr(X, beta, pct=10):\n",
    "    v=X.sum(axis=0) if not sp.issparse(X) else np.asarray(X.sum(0)).ravel()\n",
    "    p=max(np.percentile(v,pct),1.0); k=beta/p\n",
    "    return np.where(v<=p,k*(p-v),0.0)\n",
    "\n",
    "def ease_aligned(X, Xtilde, l1=1.0, beta=1.0, alpha=1.0, XtX=None):\n",
    "    n=X.shape[1]; dr=compute_dr(X,beta)\n",
    "    Xt_dr=(alpha*Xtilde)*dr[np.newaxis,:]\n",
    "    XtXt=X.T@Xt_dr\n",
    "    if XtX is None: XtX=X.T@X\n",
    "    P=np.linalg.inv(XtX+l1*np.eye(n)+XtXt)\n",
    "    Bt=P@(XtX+XtXt); g=np.diag(Bt)/np.diag(P)\n",
    "    return Bt-P@np.diag(g)\n",
    "\n",
    "# ============ EVALUATION ============\n",
    "def _topk(arr, k, axis=1):\n",
    "    return bn.argpartition(-arr,k,axis=axis) if _USE_BN else np.argpartition(-arr,k,axis=axis)\n",
    "\n",
    "def hr_at_k(pred, held, k):\n",
    "    n=pred.shape[0]\n",
    "    if k>=pred.shape[1]: k=pred.shape[1]-1\n",
    "    idx=_topk(pred,k); pb=np.zeros_like(pred,dtype=bool)\n",
    "    pb[np.arange(n)[:,None],idx[:,:k]]=True\n",
    "    tb=(held>0).toarray() if sp.issparse(held) else (held>0)\n",
    "    h=np.logical_and(tb,pb).sum(1).astype(np.float64)\n",
    "    nr=tb.sum(1).astype(np.float64)\n",
    "    return h/np.maximum(np.minimum(k,nr),1.0)\n",
    "\n",
    "def ndcg_at_k(pred, held, k):\n",
    "    n=pred.shape[0]\n",
    "    if k>=pred.shape[1]: k=pred.shape[1]-1\n",
    "    ip=_topk(pred,k); tp_part=pred[np.arange(n)[:,None],ip[:,:k]]\n",
    "    isort=np.argsort(-tp_part,axis=1); itop=ip[np.arange(n)[:,None],isort]\n",
    "    w=1.0/np.log2(np.arange(2,k+2))\n",
    "    ha=held.toarray() if sp.issparse(held) else held\n",
    "    DCG=(ha[np.arange(n)[:,None],itop]*w).sum(1)\n",
    "    nr=(ha>0).sum(1)\n",
    "    IDCG=np.array([w[:min(int(r),k)].sum() for r in nr])\n",
    "    return DCG/np.maximum(IDCG,1e-10)\n",
    "\n",
    "def evaluate(pred, test, train, ks=(10,25), cold_items=None):\n",
    "    pred=pred.copy(); co=train.tocoo(); pred[co.row,co.col]=-np.inf\n",
    "    if cold_items is not None:\n",
    "        warm=np.setdiff1d(np.arange(pred.shape[1]),cold_items); pred[:,warm]=-np.inf\n",
    "    tu=np.where(np.asarray(test.sum(1)).ravel()>0)[0]\n",
    "    if len(tu)==0: return {f\"{m}@{k}\":0.0 for k in ks for m in (\"hr\",\"ndcg\")}\n",
    "    p,t=pred[tu],test[tu]\n",
    "    r={}\n",
    "    for k in ks:\n",
    "        if k>=p.shape[1]: continue\n",
    "        r[f\"hr@{k}\"]=float(np.nanmean(hr_at_k(p,t,k)))\n",
    "        r[f\"ndcg@{k}\"]=float(np.nanmean(ndcg_at_k(p,t,k)))\n",
    "    return r\n",
    "\n",
    "# ============ SPLITS ============\n",
    "def create_cold_split(URM, cold_frac=0.20, seed=42):\n",
    "    rng=np.random.RandomState(seed); n_u,n_i=URM.shape\n",
    "    ci=np.sort(rng.choice(n_i,int(n_i*cold_frac),replace=False)); cs=set(ci)\n",
    "    coo=URM.tocoo(); tm=np.array([c not in cs for c in coo.col])\n",
    "    tr=sp.csr_matrix((coo.data[tm],(coo.row[tm],coo.col[tm])),shape=(n_u,n_i))\n",
    "    te=sp.csr_matrix((coo.data[~tm],(coo.row[~tm],coo.col[~tm])),shape=(n_u,n_i))\n",
    "    return tr,te,ci\n",
    "\n",
    "def gen_splits(URM, n=5, cf=0.20, seed=42):\n",
    "    return [{\"train\":t,\"test\":te,\"cold_items\":ci,\"seed\":seed+i}\n",
    "            for i,(t,te,ci) in enumerate([create_cold_split(URM,cf,seed+i) for i in range(n)])]\n",
    "\n",
    "print(\"All core functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Runner (optimized)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(train_csr, feat_names, ds, tag_mode, hp, min_count=2, shuffle_tags=False):\n",
    "    needs_tags = any(\"tags\" in f for f in feat_names)\n",
    "    atm = tag_mode if needs_tags else \"no_tags\"\n",
    "    enc = build_feature_matrices(\n",
    "        ds[\"dfs\"], ds[\"item2idx\"], ds[\"idx2item\"], ds[\"n_items\"],\n",
    "        tag_mode=atm,\n",
    "        train_csr=train_csr if atm not in (\"no_tags\",\"tags_full\") else None,\n",
    "        user2idx=ds[\"user2idx\"] if atm not in (\"no_tags\",\"tags_full\") else None,\n",
    "        min_count=min_count, shuffle_tags=shuffle_tags)\n",
    "    ts = enc.pop(\"_tag_stats\", None)\n",
    "    S = build_sims(enc)\n",
    "    sims = [S[f] for f in feat_names if f in S]\n",
    "    # fallback: partial match\n",
    "    if len(sims) < len(feat_names):\n",
    "        for f in feat_names:\n",
    "            if f not in S:\n",
    "                for sn in S:\n",
    "                    if f in sn: sims.append(S[sn]); break\n",
    "    X = train_csr.toarray().astype(np.float64)\n",
    "    XtX = X.T @ X\n",
    "    XG = [X @ G for G in sims]\n",
    "    y = X.ravel()\n",
    "    Xr = np.column_stack([xg.ravel() for xg in XG])\n",
    "    w = np.where(y > 0, 1.0, 0.2)\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    reg.fit(Xr, y, sample_weight=w)\n",
    "    Xtilde = sum(c * xg for c, xg in zip(reg.coef_, XG))\n",
    "    B = ease_aligned(X, Xtilde, l1=hp[\"l1\"], beta=hp[\"beta\"], alpha=hp[\"alpha\"], XtX=XtX)\n",
    "    return X @ B, ts\n",
    "\n",
    "def run_exp(ds, splits, feat_names, tag_mode, name,\n",
    "            hp_grid=None, eval_ks=(10,25), shuffle_tags=False):\n",
    "    \"\"\"Optimized: small HP grid, tune on split 0 only, evaluate on all splits.\"\"\"\n",
    "    if hp_grid is None:\n",
    "        # Focused grid based on known-good HPs: l1={10,100}, beta={1,10}, alpha={0.1,1}\n",
    "        hp_grid = {\"l1\": [10, 100], \"beta\": [1, 10], \"alpha\": [0.1, 1]}\n",
    "    n_splits = len(splits)\n",
    "    combos = list(iterproduct(hp_grid[\"l1\"], hp_grid[\"beta\"], hp_grid[\"alpha\"]))\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {name} | feats={feat_names} | tag_mode={tag_mode}\"\n",
    "          f\"{'  [SHUFFLED]' if shuffle_tags else ''}\")\n",
    "    print(f\"  HP grid: {len(combos)} combos, tune split: [0], eval splits: {n_splits}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    t0 = time.time()\n",
    "    # Tune on split 0 only\n",
    "    best_score, best_hp = -1, {\"l1\": 100, \"beta\": 1, \"alpha\": 0.1}\n",
    "    for l1, beta, alpha in combos:\n",
    "        hp = {\"l1\": l1, \"beta\": beta, \"alpha\": alpha}\n",
    "        pred, _ = predict(splits[0][\"train\"], feat_names, ds, tag_mode, hp,\n",
    "                          shuffle_tags=shuffle_tags)\n",
    "        m = evaluate(pred, splits[0][\"test\"], splits[0][\"train\"],\n",
    "                     ks=[eval_ks[0]], cold_items=splits[0][\"cold_items\"])\n",
    "        s = m.get(f\"hr@{eval_ks[0]}\", 0)\n",
    "        if s > best_score:\n",
    "            best_score = s; best_hp = hp\n",
    "    print(f\"  Best HP: l1={best_hp['l1']}, beta={best_hp['beta']}, \"\n",
    "          f\"alpha={best_hp['alpha']} (tune={best_score:.4f}, {time.time()-t0:.0f}s)\")\n",
    "    # Evaluate all splits\n",
    "    t1 = time.time()\n",
    "    per_split = []; tag_stats = None\n",
    "    for si in range(n_splits):\n",
    "        pred, ts = predict(splits[si][\"train\"], feat_names, ds, tag_mode,\n",
    "                           best_hp, shuffle_tags=shuffle_tags)\n",
    "        if ts and tag_stats is None: tag_stats = ts\n",
    "        m = evaluate(pred, splits[si][\"test\"], splits[si][\"train\"],\n",
    "                     ks=eval_ks, cold_items=splits[si][\"cold_items\"])\n",
    "        per_split.append(m)\n",
    "    avg = {k: np.mean([m[k] for m in per_split]) for k in per_split[0]}\n",
    "    std = {k: np.std([m[k] for m in per_split]) for k in per_split[0]}\n",
    "    total_time = time.time() - t0\n",
    "    print(f\"  Results ({n_splits} splits, {time.time()-t1:.0f}s):\")\n",
    "    for k in sorted(avg): print(f\"     {k}: {avg[k]:.4f} +/- {std[k]:.4f}\")\n",
    "    print(f\"  Total time: {total_time:.0f}s\")\n",
    "    return {\"name\": name, \"feats\": feat_names, \"tag_mode\": tag_mode,\n",
    "            \"best_hp\": best_hp, \"avg\": avg, \"std\": std,\n",
    "            \"per_split\": per_split, \"tag_stats\": tag_stats,\n",
    "            \"shuffle\": shuffle_tags, \"time\": total_time}\n",
    "\n",
    "print(\"Experiment runner defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Generate Splits"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "ds = load_dataset(\"hetrec_data\", threshold=3.0)\n",
    "\n",
    "N_SPLITS = 5\n",
    "print(f\"\\nGenerating {N_SPLITS} cold-start splits...\")\n",
    "splits = gen_splits(ds[\"URM\"], n=N_SPLITS, cf=0.20, seed=42)\n",
    "\n",
    "# Verify\n",
    "s0 = splits[0]\n",
    "cs = set(s0[\"cold_items\"]); tc = set(s0[\"train\"].tocoo().col)\n",
    "assert len(tc & cs) == 0, \"LEAK!\"\n",
    "print(f\"Splits OK. Cold items per split: {len(s0['cold_items'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Statistics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "tag_df = ds[\"dfs\"][\"tags\"]; ut_df = ds[\"dfs\"][\"user_tags\"]\n",
    "print(f\"movie_tags.dat: {len(tag_df)} rows, cols: {tag_df.columns.tolist()}\")\n",
    "print(f\"user_taggedmovies: {len(ut_df)} rows, cols: {ut_df.columns.tolist()}\")\n",
    "print(f\"Unique tags: {tag_df['tagID'].nunique()}\")\n",
    "print(f\"Items with tags: {tag_df['movieID'].nunique()} / {ds['n_items']} \"\n",
    "      f\"({tag_df['movieID'].nunique()/ds['n_items']*100:.1f}%)\")\n",
    "print(f\"Users who tagged: {ut_df['userID'].nunique()}\")\n",
    "# Cold-item tag coverage\n",
    "ci0 = set(str(ds[\"idx2item\"][i]) for i in splits[0][\"cold_items\"])\n",
    "tagged = set(tag_df[\"movieID\"].astype(str).unique())\n",
    "cwt = ci0 & tagged\n",
    "print(f\"Cold items with tags (split 0): {len(cwt)}/{len(ci0)} ({len(cwt)/len(ci0)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ALL Experiments\n",
    "**Estimated total: ~60 minutes** (8 HP combos per config × ~44s each + 5 eval splits)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = {}\n",
    "t_total = time.time()\n",
    "\n",
    "# 1. Baseline: top3 no_tags\n",
    "results[\"top3_no_tags\"] = run_exp(ds, splits,\n",
    "    [\"actors\",\"directors\",\"genres\"], \"no_tags\", \"top3_no_tags\")\n",
    "\n",
    "# 2. Tags train users (leakage-safe)\n",
    "results[\"top3_tags_safe\"] = run_exp(ds, splits,\n",
    "    [\"actors\",\"directors\",\"genres\",\"tags\"], \"tags_train_users\", \"top3_tags_safe\")\n",
    "\n",
    "# 3. Tags full (upper bound)\n",
    "results[\"top3_tags_full\"] = run_exp(ds, splits,\n",
    "    [\"actors\",\"directors\",\"genres\",\"tags\"], \"tags_full\", \"top3_tags_full\")\n",
    "\n",
    "# 4-6. Individual features\n",
    "for feat in [\"actors\",\"directors\",\"genres\"]:\n",
    "    results[f\"single_{feat}\"] = run_exp(ds, splits, [feat], \"no_tags\", f\"single_{feat}\")\n",
    "\n",
    "# 7. Tags only (leakage-safe)\n",
    "results[\"single_tags\"] = run_exp(ds, splits,\n",
    "    [\"tags\"], \"tags_train_users\", \"single_tags\")\n",
    "\n",
    "# 8. Top3 + countries\n",
    "results[\"top3_countries\"] = run_exp(ds, splits,\n",
    "    [\"actors\",\"directors\",\"genres\",\"countries\"], \"no_tags\", \"top3_countries\")\n",
    "\n",
    "# 9. Top3 + locations\n",
    "results[\"top3_locations\"] = run_exp(ds, splits,\n",
    "    [\"actors\",\"directors\",\"genres\",\"locations\"], \"no_tags\", \"top3_locations\")\n",
    "\n",
    "# 10. Top3 + years\n",
    "results[\"top3_years\"] = run_exp(ds, splits,\n",
    "    [\"actors\",\"directors\",\"genres\",\"years\"], \"no_tags\", \"top3_years\")\n",
    "\n",
    "# 11. Full 9-feature no tags\n",
    "results[\"base9_no_tags\"] = run_exp(ds, splits,\n",
    "    [\"genres\",\"actors\",\"directors\",\"countries\",\"loc1\",\"loc2\",\"loc3\",\"years\",\"locations\"],\n",
    "    \"no_tags\", \"base9_no_tags\")\n",
    "\n",
    "# 12. Full 9-feature + tags\n",
    "results[\"base9_tags_safe\"] = run_exp(ds, splits,\n",
    "    [\"genres\",\"actors\",\"directors\",\"countries\",\"loc1\",\"loc2\",\"loc3\",\"years\",\"locations\",\"tags\"],\n",
    "    \"tags_train_users\", \"base9_tags_safe\")\n",
    "\n",
    "# 13. CONTROL: Shuffled tags\n",
    "results[\"tags_shuffled\"] = run_exp(ds, splits,\n",
    "    [\"actors\",\"directors\",\"genres\",\"tags\"], \"tags_train_users\", \"tags_shuffled\",\n",
    "    shuffle_tags=True)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  ALL {len(results)} EXPERIMENTS DONE in {time.time()-t_total:.0f}s\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Results Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "order = [\"single_actors\",\"single_directors\",\"single_genres\",\n",
    "         \"top3_no_tags\",\"top3_countries\",\"top3_locations\",\"top3_years\",\n",
    "         \"base9_no_tags\",\n",
    "         \"single_tags\",\"top3_tags_safe\",\"base9_tags_safe\",\n",
    "         \"top3_tags_full\",\"tags_shuffled\"]\n",
    "\n",
    "bl = results[\"top3_no_tags\"][\"avg\"].get(\"hr@10\", 0)\n",
    "rows = []\n",
    "for n in order:\n",
    "    if n not in results: continue\n",
    "    r = results[n]; a = r[\"avg\"]; s = r[\"std\"]\n",
    "    delta = ((a.get(\"hr@10\",0)/bl)-1)*100 if bl>0 else 0\n",
    "    rows.append({\n",
    "        \"Config\": n,\n",
    "        \"Features\": \", \".join(r[\"feats\"]),\n",
    "        \"Tag Mode\": r[\"tag_mode\"] + (\" [SHUF]\" if r.get(\"shuffle\") else \"\"),\n",
    "        \"hr@10\": f\"{a.get('hr@10',0):.4f} +/- {s.get('hr@10',0):.4f}\",\n",
    "        \"ndcg@10\": f\"{a.get('ndcg@10',0):.4f} +/- {s.get('ndcg@10',0):.4f}\",\n",
    "        \"Delta\": f\"{delta:+.1f}%\",\n",
    "        \"Time(s)\": f\"{r.get('time',0):.0f}\",\n",
    "    })\n",
    "print(tabulate(pd.DataFrame(rows), headers=\"keys\", tablefmt=\"grid\", showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "bl_scores = [m.get(\"hr@10\",0) for m in results[\"top3_no_tags\"][\"per_split\"]]\n",
    "sig_rows = []\n",
    "for n in order:\n",
    "    if n not in results or n==\"top3_no_tags\": continue\n",
    "    other = [m.get(\"hr@10\",0) for m in results[n][\"per_split\"]]\n",
    "    if len(bl_scores)>1 and len(other)>1:\n",
    "        t_stat, p_val = stats.ttest_rel(other, bl_scores)\n",
    "        sig = \"***\" if p_val<0.001 else \"**\" if p_val<0.01 else \"*\" if p_val<0.05 else \"ns\"\n",
    "        sig_rows.append({\"Config\": n,\n",
    "            \"Mean Delta\": f\"{np.mean(other)-np.mean(bl_scores):+.4f}\",\n",
    "            \"t-stat\": f\"{t_stat:.3f}\", \"p-value\": f\"{p_val:.4f}\", \"Sig\": sig})\n",
    "print(tabulate(pd.DataFrame(sig_rows), headers=\"keys\", tablefmt=\"grid\", showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "cmap = {\"no_tags\":\"#4A90D9\",\"tags_train_users\":\"#50C878\",\"tags_full\":\"#FFB347\"}\n",
    "names, means, stds, clrs = [], [], [], []\n",
    "for n in order:\n",
    "    if n not in results: continue\n",
    "    r = results[n]; names.append(n)\n",
    "    means.append(r[\"avg\"].get(\"hr@10\",0)); stds.append(r[\"std\"].get(\"hr@10\",0))\n",
    "    clrs.append(\"#FF6B6B\" if r.get(\"shuffle\") else cmap.get(r[\"tag_mode\"],\"#4A90D9\"))\n",
    "bars = ax.bar(range(len(names)), means, yerr=stds, color=clrs, edgecolor=\"black\", linewidth=0.5, capsize=3)\n",
    "ax.axhline(y=bl, color=\"gray\", linestyle=\"--\", alpha=0.7, label=f\"Baseline ({bl:.3f})\")\n",
    "ax.set_xticks(range(len(names))); ax.set_xticklabels(names, rotation=45, ha=\"right\", fontsize=9)\n",
    "ax.set_ylabel(\"HR@10\"); ax.set_title(\"MARec Cold-Start: Feature Ablation & Tag Modes\"); ax.legend()\n",
    "ax.set_ylim(0, max(means)*1.15)\n",
    "for b,v in zip(bars,means): ax.text(b.get_x()+b.get_width()/2,b.get_height()+0.005,f\"{v:.3f}\",ha=\"center\",va=\"bottom\",fontsize=8)\n",
    "plt.tight_layout(); plt.savefig(\"hr10_all.png\",dpi=150,bbox_inches=\"tight\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Per-split variance\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "for n in [\"top3_no_tags\",\"top3_tags_safe\",\"top3_tags_full\",\"tags_shuffled\"]:\n",
    "    if n not in results: continue\n",
    "    r = results[n]; vals = [m.get(\"hr@10\",0) for m in r[\"per_split\"]]\n",
    "    label = n + (\" [CONTROL]\" if r.get(\"shuffle\") else \"\")\n",
    "    ax.plot(range(len(vals)), vals, \"o-\", label=label, markersize=6)\n",
    "ax.set_xlabel(\"Split\"); ax.set_ylabel(\"HR@10\"); ax.set_title(\"Per-Split Stability\")\n",
    "ax.legend(); ax.grid(True, alpha=0.3); plt.tight_layout()\n",
    "plt.savefig(\"per_split.png\",dpi=150,bbox_inches=\"tight\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tag mode comparison (side by side)\n",
    "fig, axes = plt.subplots(1,2,figsize=(14,6))\n",
    "for ai, metric in enumerate([\"hr@10\",\"ndcg@10\"]):\n",
    "    ax=axes[ai]; ns=[]; ms=[]; ss=[]; cs=[]\n",
    "    for n in [\"top3_no_tags\",\"top3_tags_safe\",\"top3_tags_full\",\"tags_shuffled\"]:\n",
    "        if n not in results: continue\n",
    "        r=results[n]; ns.append(n.replace(\"top3_\",\"\")); ms.append(r[\"avg\"].get(metric,0))\n",
    "        ss.append(r[\"std\"].get(metric,0))\n",
    "        cs.append(\"#FF6B6B\" if r.get(\"shuffle\") else cmap.get(r[\"tag_mode\"],\"#4A90D9\"))\n",
    "    bars=ax.bar(range(len(ns)),ms,yerr=ss,color=cs,edgecolor=\"black\",linewidth=0.5,capsize=4)\n",
    "    ax.set_xticks(range(len(ns))); ax.set_xticklabels(ns,rotation=30,ha=\"right\")\n",
    "    ax.set_ylabel(metric); ax.set_title(f\"{metric} by Tag Mode\")\n",
    "    ax.set_ylim(0,max(ms)*1.2)\n",
    "    for b,v in zip(bars,ms): ax.text(b.get_x()+b.get_width()/2,b.get_height()+0.005,f\"{v:.3f}\",ha=\"center\",va=\"bottom\",fontsize=9)\n",
    "plt.tight_layout(); plt.savefig(\"tag_modes.png\",dpi=150,bbox_inches=\"tight\"); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "out = Path(\"results/fast_experiments\"); out.mkdir(parents=True, exist_ok=True)\n",
    "rows_csv = []\n",
    "for n, r in results.items():\n",
    "    row = {\"config\": n, \"tag_mode\": r[\"tag_mode\"], \"features\": \"|\".join(r[\"feats\"]),\n",
    "           \"shuffled\": r.get(\"shuffle\", False), \"time_s\": r.get(\"time\", 0)}\n",
    "    row.update(r[\"best_hp\"]); row.update(r[\"avg\"])\n",
    "    row.update({f\"{k}_std\": v for k, v in r[\"std\"].items()})\n",
    "    rows_csv.append(row)\n",
    "pd.DataFrame(rows_csv).to_csv(out/\"all_results.csv\", index=False)\n",
    "pd.DataFrame([{\"config\":n,\"split\":si,**m} for n,r in results.items()\n",
    "              for si,m in enumerate(r[\"per_split\"])]).to_csv(out/\"per_split.csv\",index=False)\n",
    "jr = {n: {\"feats\":r[\"feats\"],\"tag_mode\":r[\"tag_mode\"],\"best_hp\":r[\"best_hp\"],\n",
    "           \"avg\":r[\"avg\"],\"std\":r[\"std\"],\"shuffle\":r.get(\"shuffle\",False)}\n",
    "      for n,r in results.items()}\n",
    "with open(out/\"results.json\",\"w\") as f: json.dump(jr,f,indent=2,default=float)\n",
    "print(f\"Saved to {out}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Dataset: {ds['n_users']} users x {ds['n_items']} items, {ds['URM'].nnz} interactions\")\n",
    "print(f\"  Splits: {N_SPLITS}, Cold: 20%\")\n",
    "print(f\"\\n  Key Results:\")\n",
    "for n in [\"top3_no_tags\",\"top3_tags_safe\",\"top3_tags_full\",\"tags_shuffled\"]:\n",
    "    if n not in results: continue\n",
    "    r=results[n]; hr=r[\"avg\"].get(\"hr@10\",0); sd=r[\"std\"].get(\"hr@10\",0)\n",
    "    d=((hr/bl)-1)*100 if bl>0 else 0\n",
    "    tag=\"[CTRL]\" if r.get(\"shuffle\") else \"[OK]  \"\n",
    "    print(f\"    {tag} {n:25s} hr@10={hr:.4f}+/-{sd:.4f}  ({d:+.1f}%)\")\n",
    "print()\n",
    "if \"tags_shuffled\" in results:\n",
    "    sh=results[\"tags_shuffled\"][\"avg\"].get(\"hr@10\",0)\n",
    "    sa=results.get(\"top3_tags_safe\",{}).get(\"avg\",{}).get(\"hr@10\",0)\n",
    "    if sh<bl*1.1: print(\"  CONTROL PASSED: Shuffled tags ~ baseline -> tags carry CONTENT signal\")\n",
    "    elif sh<sa*0.9: print(\"  CONTROL PASSED: Shuffled << real tags -> content signal confirmed\")\n",
    "    else: print(\"  WARNING: Shuffled tags still elevated -> investigate density effect\")\n",
    "print(f\"\\n  Total experiments: {len(results)}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ]
}